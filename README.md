# ğŸµ VoiceVibe - AI Audio Mood Analyzer

VoiceVibe is a professional AI-powered audio analysis tool that transcribes speech, identifies different speakers, and performs comprehensive emotional analysis. Think of it as "MoodWatch but for sound" - it analyzes the emotional tone, energy level, and overall vibe of audio recordings.

## ğŸ—ï¸ Project Structure

```
voicevibe/
â”œâ”€â”€ app.py                 # Main application entry point
â”œâ”€â”€ config.py              # Configuration settings
â”œâ”€â”€ requirements.txt       # Python dependencies
â”œâ”€â”€ env.example           # Environment variables template
â”œâ”€â”€ README.md             # This file
â”œâ”€â”€ app_old.py            # Original monolithic version (backup)
â”œâ”€â”€ audio_analyzer/       # Core application package
â”‚   â”œâ”€â”€ __init__.py       # Application factory
â”‚   â”œâ”€â”€ routes.py         # Flask routes and request handling
â”‚   â”œâ”€â”€ models.py         # Audio processing and AI models
â”‚   â””â”€â”€ utils.py          # Utility functions
â”œâ”€â”€ templates/            # HTML templates
â”‚   â””â”€â”€ index.html        # Main UI template
â”œâ”€â”€ static/               # Static assets
â”‚   â”œâ”€â”€ css/
â”‚   â”‚   â””â”€â”€ style.css     # Application styles
â”‚   â”œâ”€â”€ js/
â”‚   â”‚   â””â”€â”€ app.js        # Frontend JavaScript
â”‚   â””â”€â”€ images/           # Images and icons
â””â”€â”€ uploads/              # Temporary upload directory
```

## âœ¨ Features

- **ğŸ¤ Audio Transcription**: Convert speech to text using OpenAI Whisper
- **ğŸ—£ï¸ Speaker Diarization**: Identify and separate different speakers (optional)
- **ğŸ­ Emotional Analysis**: Comprehensive mood and emotion detection including:
  - Primary and secondary emotions
  - Energy levels (Low, Medium, High)
  - Speaking tone analysis
  - Stress indicators
  - Emotional intensity measurement
  - Key emotional phrases extraction
- **ğŸ“± Modern UI**: Beautiful, responsive web interface with drag-and-drop file upload
- **ğŸŒ Multi-format Support**: MP3, WAV, M4A, FLAC, OGG audio files
- **ğŸ”§ Professional Architecture**: Modular Flask application with proper separation of concerns
- **ğŸš€ API Endpoints**: RESTful API for programmatic access

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8 or higher
- OpenAI API key
- (Optional) HuggingFace token for speaker diarization

### Installation

1. **Clone or download this project**

2. **Install dependencies**:

   ```bash
   pip install -r requirements.txt
   ```

3. **Set up environment variables**:

   ```bash
   # Copy the example environment file
   cp env.example .env

   # Edit .env with your actual values
   nano .env
   ```

   Or set environment variables directly:

   ```bash
   export OPENAI_API_KEY="your-openai-api-key-here"
   export HF_TOKEN="your-huggingface-token-here"  # Optional, for speaker diarization
   ```

4. **Run the application (Flask + OpenFace)**:

   ```bash
   python flask_app.py
   ```

5. **Open your browser** and go to: `http://127.0.0.1:5002`

Note: The project now uses OpenFace for facial Action Units and emotion estimation in the video UI. MediaPipe paths are deprecated and kept only as stubs.

## ğŸ¯ How to Use

1. **Upload Audio**: Click or drag your audio file into the upload area
2. **Optional**: Check "Enable Speaker Separation" if you want to identify different speakers
3. **Analyze**: Click "Analyze Audio" to process your file
4. **View Results**: Get comprehensive analysis including:
   - Full transcript
   - Detailed emotional analysis
   - Speaker timeline (if enabled)

## ğŸ“‹ API Keys Setup

### OpenAI API Key (Required)

1. Go to [OpenAI API Keys](https://platform.openai.com/account/api-keys)
2. Create a new API key
3. Set it as environment variable: `OPENAI_API_KEY`

### HuggingFace Token (Optional - for Speaker Diarization)

1. Go to [HuggingFace Settings](https://huggingface.co/settings/tokens)
2. Create a new token
3. Accept the terms for `pyannote/speaker-diarization` model
4. Set it as environment variable: `HF_TOKEN`

## ğŸ”§ Configuration

You can modify these settings in `app.py`:

- `CHAT_MODEL`: OpenAI model for emotion analysis (default: "gpt-4o-mini")
- `ENABLE_DIARIZATION_DEFAULT`: Default state for speaker diarization checkbox

## ğŸ“Š Emotional Analysis Details

VoiceVibe provides comprehensive emotional analysis including:

- **Primary Emotion**: Main detected emotion (Happy, Sad, Angry, Excited, Calm, Anxious, etc.)
- **Secondary Emotions**: Additional emotions detected
- **Mood Category**: Overall mood classification (Positive, Negative, Neutral, Mixed)
- **Energy Level**: Speaking energy (Low, Medium, High)
- **Tone**: Communication style (Formal, Casual, Emotional, etc.)
- **Stress Indicators**: Signs of stress (Fast speech, Repetition, Filler words, Hesitation)
- **Emotional Intensity**: How intense the emotions are (0-100%)
- **Key Phrases**: Important emotional phrases from the transcript
- **Overall Vibe**: Casual description of the overall feeling

## ğŸ› ï¸ Troubleshooting

### Common Issues

1. **"No speech detected"**: Ensure your audio file contains clear speech
2. **API errors**: Check your OpenAI API key and account limits
3. **Speaker diarization not working**: Install pyannote.audio and set HF_TOKEN
4. **File upload issues**: Ensure your audio file is in a supported format

### File Size Limits

- Maximum file size depends on your OpenAI plan
- For large files, consider splitting them into smaller chunks

## ğŸµ Supported Audio Formats

- MP3
- WAV
- M4A
- FLAC
- OGG

## ğŸ¤ Contributing

This project was built based on conversation requirements for creating an audio analysis tool that can "rip" audio files, separate speakers, and analyze emotional content like MoodWatch does for visual content.

## ğŸ“„ License

This project is open source. Feel free to modify and use it for your needs.

## ğŸ”Œ API Endpoints

VoiceVibe provides RESTful API endpoints for programmatic access:

### POST `/api/analyze`

Analyze audio file and return JSON results.

**Parameters:**

- `audio`: Audio file (multipart/form-data)
- `diarize`: Boolean, enable speaker diarization (optional)

**Response:**

```json
{
  "success": true,
  "result": {
    "transcript": "...",
    "primary_emotion": "Happy",
    "secondary_emotions": ["Excited"],
    "mood_category": "Positive",
    "energy_level": "High",
    "tone": "Casual",
    "confidence": 0.87,
    "stress_indicators": [],
    "emotional_intensity": 0.75,
    "key_phrases": ["great news", "excited to share"],
    "overall_vibe": "Enthusiastic and positive",
    "explanation": "...",
    "diarization": [...]
  }
}
```

### GET `/health`

Health check endpoint.

### GET `/config`

Get public configuration information.

## ğŸ§ª Development

### Project Architecture

- **Flask Application Factory**: Modular app creation with configuration management
- **Blueprint Structure**: Organized route handling
- **Separation of Concerns**: Models, utils, routes, and templates are separate
- **Professional Error Handling**: User-friendly error messages and logging
- **Static Asset Management**: Organized CSS, JS, and image files

### Adding New Features

1. **Models**: Add audio processing functions to `audio_analyzer/models.py`
2. **Routes**: Add new endpoints to `audio_analyzer/routes.py`
3. **Frontend**: Update templates and static files
4. **Configuration**: Update `config.py` for new settings

## ğŸ†˜ Support

If you encounter issues:

1. Check that all environment variables are set correctly
2. Ensure you have a stable internet connection
3. Verify your audio file format is supported
4. Check your OpenAI API account status and limits
5. Review the logs for detailed error information

### Common Issues

- **Import errors**: Ensure all dependencies are installed via `pip install -r requirements.txt`
- **API key errors**: Double-check your OpenAI API key is valid and has sufficient credits
- **Diarization issues**: Verify HuggingFace token and model access permissions

---

Built with â¤ï¸ using Flask, OpenAI Whisper, and modern web technologies.
