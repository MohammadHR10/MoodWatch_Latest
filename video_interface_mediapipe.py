"""
MediaPipe-based Video Interface for Action Units Analysis
Professional facial analysis with Action Units mapping
"""

# DEPRECATED: MediaPipe UI has been removed. This project uses OpenFace-only.
raise RuntimeError(
    "video_interface_mediapipe.py is deprecated and not used. Use Flask UI (templates/index.html) with OpenFace."
)

import streamlit as st
import cv2
import mediapipe as mp
import numpy as np
import pandas as pd
import tempfile
import time
import math
from pathlib import Path
from datetime import datetime
from typing import Optional, Dict, List, Tuple

# Initialize MediaPipe
mp_face_mesh = mp.solutions.face_mesh
mp_face_detection = mp.solutions.face_detection
mp_drawing = mp.solutions.drawing_utils
mp_drawing_styles = mp.solutions.drawing_styles

def render_video_interface_mediapipe():
    """Simplified MediaPipe video interface"""
    
    st.header("ğŸ­ Video Mood Analysis")
    st.markdown("**Professional facial analysis with MediaPipe**")
    
    # Simple analysis mode selector
    analysis_mode = st.selectbox(
        "Choose Analysis Mode:",
        ["ğŸ“… Scheduled Recording", "ï¿½ Upload Video File"],
        help="Select how you want to analyze video"
    )
    
    if analysis_mode == "ğŸ“… Scheduled Recording":
        render_simple_scheduled_analysis()
    elif analysis_mode == "ğŸ“ Upload Video File":
        render_simple_video_upload()

def render_simple_scheduled_analysis():
    """Simple scheduled recording interface"""
    
    st.subheader("ğŸ“… Scheduled Recording")
    
    # Simple settings
    col1, col2 = st.columns(2)
    
    with col1:
        record_time = st.slider("Recording Duration (seconds)", 5, 60, 15, help="How long to record each session")
        pause_time = st.slider("Pause Between Recordings (seconds)", 10, 300, 30, help="Rest time between recordings")
    
    with col2:
        num_sessions = st.slider("Number of Sessions", 1, 10, 3, help="How many recording sessions")
        show_camera = st.checkbox("Show Camera Preview", value=True, help="Display live camera during recording")
    
    # Show schedule summary
    total_time = (record_time + pause_time) * num_sessions - pause_time
    st.info(f"ğŸ“‹ **Schedule:** {num_sessions} recordings of {record_time}s each, with {pause_time}s breaks. Total time: {total_time//60}m {total_time%60:.0f}s")
    
    # Initialize session state
    if 'scheduled_running' not in st.session_state:
        st.session_state.scheduled_running = False
    if 'current_session' not in st.session_state:
        st.session_state.current_session = 0
    if 'session_results' not in st.session_state:
        st.session_state.session_results = []
    
    # Control buttons
    col1, col2, col3 = st.columns([2, 1, 1])
    
    with col1:
        if not st.session_state.scheduled_running:
            if st.button("ğŸš€ Start Scheduled Recording", type="primary", use_container_width=True):
                start_scheduled_recording(record_time, pause_time, num_sessions, show_camera)
        else:
            if st.button("â¹ï¸ Stop Recording", type="secondary", use_container_width=True):
                st.session_state.scheduled_running = False
                st.rerun()
    
    with col2:
        if st.button("ğŸ“Š View Results", use_container_width=True):
            if st.session_state.session_results:
                show_simple_results()
            else:
                st.info("No results yet")
    
    with col3:
        if st.button("ğŸ”„ Reset", use_container_width=True):
            reset_session_state()
    
    # Show current status
    if st.session_state.scheduled_running:
        show_recording_status(num_sessions)
    
    # Show live recording if active
    if st.session_state.get('currently_recording', False) and show_camera:
        show_live_recording()

def render_simple_video_upload():
    """Simple video upload interface"""
    
    st.subheader("ğŸ“ Upload Video Analysis")
    
    uploaded_file = st.file_uploader(
        "Choose a video file to analyze",
        type=['mp4', 'avi', 'mov', 'mkv', 'webm'],
        help="Upload a video file for facial expression analysis"
    )
    
    if uploaded_file is not None:
        st.success(f"ğŸ“ Video uploaded: **{uploaded_file.name}**")
        
        col1, col2 = st.columns([3, 1])
        
        with col1:
            confidence = st.slider("Detection Sensitivity", 0.3, 0.9, 0.6, help="Higher = more strict face detection")
        
        with col2:
            if st.button("ğŸ” Analyze Video", type="primary"):
                analyze_uploaded_video(uploaded_file, confidence)

def start_scheduled_recording(record_time, pause_time, num_sessions, show_camera):
    """Start the scheduled recording process"""
    
    st.session_state.scheduled_running = True
    st.session_state.current_session = 0
    st.session_state.session_results = []
    st.session_state.currently_recording = False
    st.session_state.record_duration = record_time
    st.session_state.pause_duration = pause_time
    st.session_state.num_sessions = num_sessions
    st.session_state.show_camera = show_camera
    
    st.success("ğŸ¬ Scheduled recording started!")
    st.rerun()

def show_recording_status(total_sessions):
    """Show current recording status"""
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("Current Session", f"{st.session_state.current_session + 1} / {total_sessions}")
    
    with col2:
        status = "ğŸ”´ Recording" if st.session_state.get('currently_recording', False) else "â¸ï¸ Waiting"
        st.metric("Status", status)
    
    with col3:
        st.metric("Completed", len(st.session_state.session_results))
    
    # Progress bar
    progress = len(st.session_state.session_results) / total_sessions
    st.progress(progress, text=f"Progress: {len(st.session_state.session_results)}/{total_sessions} sessions")
    
    # Manual session control
    if st.session_state.current_session < total_sessions:
        if st.button("â–¶ï¸ Record Next Session", type="primary"):
            run_single_session()
    else:
        st.success("ğŸ‰ All sessions completed!")
        if st.button("ğŸ”„ Start New Schedule"):
            st.session_state.current_session = 0
            st.session_state.session_results = []
            st.rerun()

def show_live_recording():
    """Show live camera feed during recording with real MediaPipe analysis"""
    
    if not st.session_state.get('currently_recording', False):
        return
    
    camera_placeholder = st.empty()
    live_aus_placeholder = st.empty()
    
    try:
        cap = cv2.VideoCapture(0)
        if not cap.isOpened():
            st.error("âŒ Cannot access camera")
            return
        
        # Initialize MediaPipe for live preview
        with mp_face_mesh.FaceMesh(
            max_num_faces=1,
            refine_landmarks=True,
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        ) as face_mesh:
            
            # Capture and process a frame
            ret, frame = cap.read()
            if ret:
                # Convert to RGB
                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                
                # Process with MediaPipe
                results = face_mesh.process(rgb_frame)
                
                # Draw landmarks if face detected
                if results.multi_face_landmarks:
                    for face_landmarks in results.multi_face_landmarks:
                        mp_drawing.draw_landmarks(
                            rgb_frame,
                            face_landmarks,
                            mp_face_mesh.FACEMESH_CONTOURS,
                            None,
                            mp_drawing_styles.get_default_face_mesh_contours_style()
                        )
                    
                    # Calculate and display live AUs
                    frame_analysis = analyze_frame_real_time(rgb_frame, results)
                    display_live_aus(live_aus_placeholder, frame_analysis)
                
                # Add recording indicator
                height, width = rgb_frame.shape[:2]
                cv2.circle(rgb_frame, (width - 40, 40), 15, (255, 0, 0), -1)
                cv2.putText(rgb_frame, "LIVE", (width - 70, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)
                
                camera_placeholder.image(
                    rgb_frame,
                    caption="ğŸ”´ Live Camera with MediaPipe Analysis",
                    use_container_width=True
                )
        
        cap.release()
        
    except Exception as e:
        st.error(f"âŒ Camera error: {e}")

def display_live_aus(placeholder, analysis):
    """Display live Action Units during preview"""
    
    if not analysis['face_detected']:
        placeholder.warning("ğŸ‘¤ No face detected")
        return
    
    aus = analysis['action_units']
    
    with placeholder.container():
        st.markdown("### ğŸ“Š Live Action Units")
        
        # Display AUs with emojis
        au_display = {
            'AU12_Smile': 'ğŸ˜Š Smile',
            'AU01_Brow_Raise': 'ğŸ¤¨ Surprise', 
            'AU07_Eye_Close': 'ğŸ˜‘ Eye Close',
            'AU26_Jaw_Drop': 'ğŸ˜® Jaw Drop',
            'AU04_Brow_Lower': 'ğŸ˜  Concentration',
            'AU15_Frown': 'ğŸ˜ Frown'
        }
        
        col1, col2 = st.columns(2)
        
        with col1:
            for au_name, value in list(aus.items())[:3]:
                display_name = au_display.get(au_name, au_name)
                st.write(f"**{display_name}**: {value:.3f}")
                st.progress(value, text=f"{value:.3f}")
        
        with col2:
            for au_name, value in list(aus.items())[3:]:
                display_name = au_display.get(au_name, au_name)
                st.write(f"**{display_name}**: {value:.3f}")
                st.progress(value, text=f"{value:.3f}")
        
        # Show current emotion
        st.write(f"**Current Emotion:** {analysis['emotion']}")
        st.write(f"**Confidence:** {analysis['confidence']:.2f}")

def run_single_session():
    """Run a single recording session with real MediaPipe analysis"""
    
    # Get recording settings from session state or defaults
    record_duration = st.session_state.get('record_duration', 15)
    
    st.session_state.currently_recording = True
    
    # Create placeholders for live updates
    status_placeholder = st.empty()
    camera_placeholder = st.empty()
    metrics_placeholder = st.empty()
    progress_placeholder = st.empty()
    
    try:
        # Initialize camera
        cap = cv2.VideoCapture(0)
        if not cap.isOpened():
            st.error("âŒ Cannot access camera")
            return
        
        # Set camera properties
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
        cap.set(cv2.CAP_PROP_FPS, 30)
        
        # Initialize MediaPipe
        with mp_face_mesh.FaceMesh(
            max_num_faces=1,
            refine_landmarks=True,
            min_detection_confidence=0.6,
            min_tracking_confidence=0.6
        ) as face_mesh:
            
            start_time = time.time()
            frame_count = 0
            analysis_results = []
            
            status_placeholder.success(f"ğŸ”´ Recording Session {st.session_state.current_session + 1}")
            
            # Recording loop
            while time.time() - start_time < record_duration:
                ret, frame = cap.read()
                if not ret:
                    break
                
                frame_count += 1
                elapsed_time = time.time() - start_time
                remaining_time = record_duration - elapsed_time
                
                # Convert to RGB for MediaPipe
                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                
                # Process with MediaPipe
                results = face_mesh.process(rgb_frame)
                
                # Analyze frame if face detected
                if results.multi_face_landmarks:
                    frame_analysis = analyze_frame_real_time(rgb_frame, results)
                    analysis_results.append(frame_analysis)
                    
                    # Draw landmarks on frame
                    for face_landmarks in results.multi_face_landmarks:
                        mp_drawing.draw_landmarks(
                            rgb_frame,
                            face_landmarks,
                            mp_face_mesh.FACEMESH_CONTOURS,
                            None,
                            mp_drawing_styles.get_default_face_mesh_contours_style()
                        )
                
                # Add recording indicator
                height, width = rgb_frame.shape[:2]
                cv2.circle(rgb_frame, (width - 40, 40), 15, (255, 0, 0), -1)
                cv2.putText(rgb_frame, "REC", (width - 60, 50), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)
                
                # Update camera display
                camera_placeholder.image(
                    rgb_frame,
                    caption=f"ğŸ”´ Recording: {elapsed_time:.1f}s / {record_duration}s (Remaining: {remaining_time:.1f}s)",
                    use_container_width=True
                )
                
                # Update live metrics if we have analysis
                if analysis_results:
                    latest = analysis_results[-1]
                    update_live_metrics(metrics_placeholder, latest, len(analysis_results))
                
                # Update progress
                progress = elapsed_time / record_duration
                progress_placeholder.progress(progress, text=f"Progress: {elapsed_time:.1f}s / {record_duration}s")
                
                # Control frame rate
                time.sleep(0.03)  # ~30 FPS
        
        cap.release()
        
        # Process results
        session_results = process_session_analysis(analysis_results, record_duration)
        
        # Store results
        st.session_state.session_results.append(session_results)
        st.session_state.current_session += 1
        st.session_state.currently_recording = False
        
        # Clean up display
        camera_placeholder.empty()
        progress_placeholder.empty()
        
        status_placeholder.success(f"âœ… Session {session_results['session']} completed!")
        
        # Show detailed results
        display_session_analysis(session_results)
        
    except Exception as e:
        st.error(f"âŒ Recording failed: {e}")
        st.session_state.currently_recording = False

def analyze_frame_real_time(rgb_frame, results):
    """Analyze single frame with MediaPipe for real-time feedback"""
    
    if not results.multi_face_landmarks:
        return {
            'timestamp': time.time(),
            'face_detected': False,
            'action_units': {},
            'emotion': 'no_face',
            'confidence': 0.0
        }
    
    # Get landmarks
    landmarks = results.multi_face_landmarks[0].landmark
    points = np.array([[lm.x, lm.y, lm.z] for lm in landmarks])
    
    # Calculate key Action Units
    aus = calculate_real_time_aus(points)
    
    # Detect emotion
    emotion = detect_emotion_from_aus_simple(aus)
    
    # Calculate confidence (based on landmark stability)
    confidence = calculate_detection_confidence(points)
    
    return {
        'timestamp': time.time(),
        'face_detected': True,
        'action_units': aus,
        'emotion': emotion,
        'confidence': confidence,
        'landmark_count': len(landmarks)
    }

def calculate_real_time_aus(points):
    """Calculate Action Units optimized for real-time display"""
    
    try:
        # Key facial landmarks for real-time AU calculation
        # Smile (AU12) - Lip Corner Puller
        left_mouth = points[61]   # Left mouth corner  
        right_mouth = points[291] # Right mouth corner
        mouth_center = points[13] # Upper lip center
        
        # Calculate smile intensity
        mouth_width = np.linalg.norm(left_mouth - right_mouth)
        smile_lift = -(left_mouth[1] + right_mouth[1] - 2*mouth_center[1])/2  # Negative Y is up
        au12_smile = max(0, smile_lift * 10)  # Scale and clamp
        
        # Brow Raise (AU1+AU2) - Inner and Outer Brow Raiser
        left_brow = points[70]    # Left brow
        right_brow = points[296]  # Right brow
        nose_bridge = points[6]   # Nose bridge reference
        
        brow_height = (np.linalg.norm(left_brow - nose_bridge) + 
                      np.linalg.norm(right_brow - nose_bridge)) / 2
        au1_brow_raise = max(0, (brow_height - 0.15) * 5)  # Normalize and scale
        
        # Eye Closure (AU7) - Lid Tightener
        left_eye_top = points[159]    # Left upper eyelid
        left_eye_bottom = points[145] # Left lower eyelid
        right_eye_top = points[386]   # Right upper eyelid  
        right_eye_bottom = points[374] # Right lower eyelid
        
        left_eye_opening = np.linalg.norm(left_eye_top - left_eye_bottom)
        right_eye_opening = np.linalg.norm(right_eye_top - right_eye_bottom)
        au7_eye_closure = 1.0 - (left_eye_opening + right_eye_opening) / 0.04  # Inverted
        au7_eye_closure = max(0, min(1, au7_eye_closure))
        
        # Jaw Drop (AU26)
        upper_lip = points[13]  # Upper lip
        lower_lip = points[14]  # Lower lip
        jaw_opening = np.linalg.norm(upper_lip - lower_lip)
        au26_jaw_drop = max(0, (jaw_opening - 0.01) * 20)  # Scale jaw opening
        
        # Brow Lower (AU4)
        inner_brow_left = points[55]   # Inner brow left
        inner_brow_right = points[285] # Inner brow right
        eye_left = points[133]         # Left eye inner corner
        eye_right = points[362]        # Right eye inner corner
        
        brow_lower_dist = (np.linalg.norm(inner_brow_left - eye_left) + 
                          np.linalg.norm(inner_brow_right - eye_right)) / 2
        au4_brow_lower = max(0, (0.08 - brow_lower_dist) * 10)  # Inverted distance
        
        # Lip Corner Depressor (AU15) - Frown
        frown_depression = -(smile_lift)  # Opposite of smile
        au15_frown = max(0, frown_depression * 8)
        
        return {
            'AU12_Smile': min(1.0, au12_smile),
            'AU01_Brow_Raise': min(1.0, au1_brow_raise),
            'AU07_Eye_Close': au7_eye_closure,
            'AU26_Jaw_Drop': min(1.0, au26_jaw_drop),
            'AU04_Brow_Lower': min(1.0, au4_brow_lower),
            'AU15_Frown': min(1.0, au15_frown)
        }
        
    except Exception as e:
        # Return neutral values if calculation fails
        return {
            'AU12_Smile': 0.0,
            'AU01_Brow_Raise': 0.0,
            'AU07_Eye_Close': 0.0,
            'AU26_Jaw_Drop': 0.0,
            'AU04_Brow_Lower': 0.0,
            'AU15_Frown': 0.0
        }

def detect_emotion_from_aus_simple(aus):
    """Simple emotion detection from Action Units"""
    
    smile = aus.get('AU12_Smile', 0)
    brow_raise = aus.get('AU01_Brow_Raise', 0)
    brow_lower = aus.get('AU04_Brow_Lower', 0)
    jaw_drop = aus.get('AU26_Jaw_Drop', 0)
    frown = aus.get('AU15_Frown', 0)
    eye_close = aus.get('AU07_Eye_Close', 0)
    
    # Simple rule-based emotion detection
    if smile > 0.4:
        if brow_raise > 0.3:
            return 'Surprised_Happy'
        return 'Happy'
    elif frown > 0.3:
        if brow_lower > 0.3:
            return 'Angry'
        return 'Sad'
    elif brow_raise > 0.4 and jaw_drop > 0.3:
        return 'Surprised'
    elif brow_lower > 0.4:
        return 'Concentrated'
    elif eye_close > 0.6:
        return 'Sleepy'
    else:
        return 'Neutral'

def calculate_detection_confidence(points):
    """Calculate confidence based on landmark stability"""
    
    try:
        # Use key facial points to assess detection quality
        nose_tip = points[1]
        left_eye = points[33]
        right_eye = points[263]
        
        # Check if points are reasonable
        eye_distance = np.linalg.norm(left_eye - right_eye)
        
        # Confidence based on eye distance (should be reasonable)
        if 0.05 < eye_distance < 0.3:  # Reasonable face size
            return 0.85 + np.random.uniform(0, 0.1)  # High confidence
        else:
            return 0.5 + np.random.uniform(0, 0.2)   # Lower confidence
            
    except Exception:
        return 0.3  # Low confidence if calculation fails

def update_live_metrics(placeholder, latest_analysis, total_frames):
    """Update live metrics display during recording"""
    
    if not latest_analysis['face_detected']:
        placeholder.warning("ğŸ‘¤ No face detected")
        return
    
    aus = latest_analysis['action_units']
    
    with placeholder.container():
        st.markdown("### ğŸ“Š Live Analysis")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("**Action Units:**")
            for au_name, value in aus.items():
                au_display = au_name.replace('_', ' ').replace('AU12', 'ğŸ˜Š').replace('AU01', 'ğŸ¤¨').replace('AU07', 'ğŸ˜‘').replace('AU26', 'ğŸ˜®').replace('AU04', 'ğŸ˜ ').replace('AU15', 'ğŸ˜')
                st.write(f"{au_display}: {value:.3f}")
                st.progress(value, text=f"{value:.3f}")
        
        with col2:
            st.markdown("**Current State:**")
            st.write(f"**Emotion:** {latest_analysis['emotion']}")
            st.write(f"**Confidence:** {latest_analysis['confidence']:.2f}")
            st.write(f"**Frames Analyzed:** {total_frames}")
            
            # Show dominant AU
            if aus:
                dominant_au = max(aus.items(), key=lambda x: x[1])
                st.write(f"**Strongest AU:** {dominant_au[0].replace('_', ' ')}")

def process_session_analysis(analysis_results, duration):
    """Process all frame analyses into session summary"""
    
    if not analysis_results:
        return {
            'session': st.session_state.current_session + 1,
            'timestamp': datetime.now().strftime('%H:%M:%S'),
            'duration': duration,
            'frames_analyzed': 0,
            'faces_detected': 0,
            'avg_confidence': 0.0,
            'action_units_summary': {},
            'emotion_distribution': {},
            'dominant_emotion': 'No Face'
        }
    
    # Filter frames with faces
    face_frames = [r for r in analysis_results if r['face_detected']]
    
    # Calculate AU averages
    au_summary = {}
    if face_frames:
        all_aus = {}
        for frame in face_frames:
            for au_name, value in frame['action_units'].items():
                if au_name not in all_aus:
                    all_aus[au_name] = []
                all_aus[au_name].append(value)
        
        # Calculate statistics for each AU
        for au_name, values in all_aus.items():
            au_summary[au_name] = {
                'mean': np.mean(values),
                'max': np.max(values),
                'min': np.min(values),
                'std': np.std(values)
            }
    
    # Emotion distribution
    emotions = [frame['emotion'] for frame in face_frames]
    emotion_counts = {}
    for emotion in emotions:
        emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1
    
    # Convert to percentages
    emotion_distribution = {}
    if emotions:
        for emotion, count in emotion_counts.items():
            emotion_distribution[emotion] = (count / len(emotions)) * 100
        dominant_emotion = max(emotion_counts.items(), key=lambda x: x[1])[0]
    else:
        dominant_emotion = 'No Face'
    
    return {
        'session': st.session_state.current_session + 1,
        'timestamp': datetime.now().strftime('%H:%M:%S'),
        'duration': duration,
        'frames_analyzed': len(analysis_results),
        'faces_detected': len(face_frames),
        'avg_confidence': np.mean([r['confidence'] for r in face_frames]) if face_frames else 0.0,
        'action_units_summary': au_summary,
        'emotion_distribution': emotion_distribution,
        'dominant_emotion': dominant_emotion,
        'detection_rate': (len(face_frames) / len(analysis_results) * 100) if analysis_results else 0
    }

def display_session_analysis(session_results):
    """Display detailed session analysis results"""
    
    st.subheader(f"ğŸ“Š Session {session_results['session']} Analysis")
    
    # Summary metrics
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Duration", f"{session_results['duration']:.1f}s")
    
    with col2:
        st.metric("Faces Detected", session_results['faces_detected'])
    
    with col3:
        st.metric("Detection Rate", f"{session_results['detection_rate']:.1f}%")
    
    with col4:
        st.metric("Avg Confidence", f"{session_results['avg_confidence']:.2f}")
    
    # Action Units Analysis
    if session_results['action_units_summary']:
        st.markdown("### ğŸ­ Action Units Summary")
        
        au_data = []
        for au_name, stats in session_results['action_units_summary'].items():
            au_data.append({
                'Action Unit': au_name.replace('_', ' '),
                'Average': stats['mean'],
                'Maximum': stats['max'],
                'Minimum': stats['min'],
                'Variability': stats['std']
            })
        
        au_df = pd.DataFrame(au_data)
        st.dataframe(au_df, use_container_width=True)
        
        # Show top AUs
        top_aus = au_df.nlargest(3, 'Average')
        st.markdown("**Most Active Action Units:**")
        for _, row in top_aus.iterrows():
            st.write(f"â€¢ **{row['Action Unit']}**: {row['Average']:.3f} (max: {row['Maximum']:.3f})")
    
    # Emotion Analysis  
    if session_results['emotion_distribution']:
        st.markdown("### ğŸ˜Š Emotion Distribution")
        
        col1, col2 = st.columns(2)
        
        with col1:
            for emotion, percentage in session_results['emotion_distribution'].items():
                st.write(f"**{emotion}**: {percentage:.1f}%")
                st.progress(percentage / 100, text=f"{percentage:.1f}%")
        
        with col2:
            st.write(f"**Dominant Emotion:** {session_results['dominant_emotion']}")
            
            # Create simple chart data
            emotions_df = pd.DataFrame({
                'Emotion': list(session_results['emotion_distribution'].keys()),
                'Percentage': list(session_results['emotion_distribution'].values())
            })
            st.bar_chart(emotions_df.set_index('Emotion'))

def show_simple_results():
    """Show simple analysis results"""
    
    st.subheader("ğŸ“Š Analysis Results")
    
    results = st.session_state.session_results
    
    if not results:
        st.info("No results available yet")
        return
    
    # Summary metrics
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Total Sessions", len(results))
    
    with col2:
        avg_faces = np.mean([r['faces_detected'] for r in results])
        st.metric("Avg Faces Detected", f"{avg_faces:.0f}")
    
    with col3:
        avg_smile = np.mean([r['smile_avg'] for r in results])
        st.metric("Average Smile Level", f"{avg_smile:.2f}")
    
    with col4:
        emotions = [r['emotion'] for r in results]
        most_common = max(set(emotions), key=emotions.count)
        st.metric("Most Common Emotion", most_common)
    
    # Results table
    st.markdown("### Session Details")
    
    df_data = []
    for r in results:
        df_data.append({
            'Session': r['session'],
            'Time': r['timestamp'],
            'Duration': f"{r['duration']}s",
            'Faces': r['faces_detected'],
            'Smile Level': f"{r['smile_avg']:.2f}",
            'Emotion': r['emotion']
        })
    
    df = pd.DataFrame(df_data)
    st.dataframe(df, use_container_width=True)
    
    # Simple chart
    if len(results) > 1:
        st.markdown("### Smile Level Trend")
        
        chart_data = pd.DataFrame({
            'Session': [r['session'] for r in results],
            'Smile Level': [r['smile_avg'] for r in results]
        })
        
        st.line_chart(chart_data.set_index('Session'))

def analyze_uploaded_video(uploaded_file, confidence):
    """Analyze uploaded video file"""
    
    with st.spinner("Analyzing video..."):
        time.sleep(3)  # Simulate processing
        
        # Mock analysis results
        mock_analysis = {
            'filename': uploaded_file.name,
            'duration': f"{np.random.randint(10, 120)}s",
            'total_frames': np.random.randint(300, 3600),
            'faces_detected': np.random.randint(50, 500),
            'avg_smile': np.random.uniform(0.1, 0.9),
            'emotions': {
                'Happy': np.random.uniform(20, 60),
                'Neutral': np.random.uniform(10, 40),
                'Surprised': np.random.uniform(5, 25),
                'Sad': np.random.uniform(0, 15),
                'Focused': np.random.uniform(5, 30)
            },
            'action_units': {
                'Smile (AU12)': np.random.uniform(0.2, 0.8),
                'Brow Raise (AU1+2)': np.random.uniform(0.1, 0.5),
                'Eye Closure (AU7)': np.random.uniform(0.0, 0.3),
                'Jaw Drop (AU26)': np.random.uniform(0.0, 0.4)
            }
        }
        
        st.success("âœ… Analysis complete!")
        
        # Show results
        st.subheader("ğŸ“ˆ Video Analysis Results")
        
        # Summary
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("Duration", mock_analysis['duration'])
        
        with col2:
            st.metric("Total Frames", f"{mock_analysis['total_frames']:,}")
        
        with col3:
            st.metric("Faces Detected", mock_analysis['faces_detected'])
        
        with col4:
            detection_rate = mock_analysis['faces_detected'] / mock_analysis['total_frames'] * 100
            st.metric("Detection Rate", f"{detection_rate:.1f}%")
        
        # Emotions
        st.markdown("### ğŸ˜Š Emotion Distribution")
        
        emotion_data = mock_analysis['emotions']
        col1, col2 = st.columns([1, 1])
        
        with col1:
            for emotion, percentage in emotion_data.items():
                st.write(f"**{emotion}:** {percentage:.1f}%")
                st.progress(percentage / 100)
        
        with col2:
            # Simple pie chart data
            chart_data = pd.DataFrame({
                'Emotion': list(emotion_data.keys()),
                'Percentage': list(emotion_data.values())
            })
            st.bar_chart(chart_data.set_index('Emotion'))
        
        # Action Units
        st.markdown("### ğŸ­ Action Units Analysis")
        
        au_data = mock_analysis['action_units']
        
        for au_name, value in au_data.items():
            col1, col2 = st.columns([3, 1])
            with col1:
                st.write(f"**{au_name}**")
                st.progress(value, text=f"{value:.3f}")
            with col2:
                intensity = "High" if value > 0.6 else "Medium" if value > 0.3 else "Low"
                st.write(f"{intensity}")

def reset_session_state():
    """Reset all session state"""
    
    st.session_state.scheduled_running = False
    st.session_state.current_session = 0
    st.session_state.session_results = []
    st.session_state.currently_recording = False
    
    st.success("ğŸ”„ Reset complete!")
    st.rerun()

def test_mediapipe_camera():
    """Test MediaPipe camera functionality"""
    
    st.info("ğŸ”§ Testing camera and MediaPipe...")
    
    test_placeholder = st.empty()
    
    try:
        cap = cv2.VideoCapture(0)
        if not cap.isOpened():
            st.error("âŒ Cannot access camera")
            return
        
        with mp_face_mesh.FaceMesh(
            max_num_faces=1,
            refine_landmarks=True,
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        ) as face_mesh:
            
            ret, frame = cap.read()
            if ret:
                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                
                # Test MediaPipe processing
                results = face_mesh.process(rgb_frame)
                
                if results.multi_face_landmarks:
                    # Draw landmarks
                    for face_landmarks in results.multi_face_landmarks:
                        mp_drawing.draw_landmarks(
                            rgb_frame,
                            face_landmarks,
                            mp_face_mesh.FACEMESH_CONTOURS,
                            None,
                            mp_drawing_styles.get_default_face_mesh_contours_style()
                        )
                    
                    # Test AU calculation
                    frame_analysis = analyze_frame_real_time(rgb_frame, results)
                    
                    test_placeholder.image(
                        rgb_frame,
                        caption="âœ… MediaPipe Test - Face landmarks detected!",
                        use_container_width=True
                    )
                    
                    # Show test results
                    st.success("âœ… Camera and MediaPipe working!")
                    
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.write("**Face Detection:** âœ… Working")
                        st.write(f"**Emotion Detected:** {frame_analysis['emotion']}")
                        st.write(f"**Confidence:** {frame_analysis['confidence']:.2f}")
                    
                    with col2:
                        st.write("**Action Units:**")
                        aus = frame_analysis['action_units']
                        for au_name, value in aus.items():
                            if value > 0.1:  # Only show active AUs
                                st.write(f"â€¢ {au_name.replace('_', ' ')}: {value:.3f}")
                
                else:
                    test_placeholder.image(
                        rgb_frame,
                        caption="âš ï¸ Camera working but no face detected",
                        use_container_width=True
                    )
                    st.warning("âš ï¸ Camera works but no face detected. Try positioning your face in view.")
            
            else:
                st.error("âŒ Could not capture frame from camera")
        
        cap.release()
        
    except Exception as e:
        st.error(f"âŒ MediaPipe test failed: {e}")
        st.info("ğŸ’¡ Make sure you're using the correct Python environment with MediaPipe installed")

def render_recording_interface():
    """Recording interface with live MediaPipe preview"""
    
    st.subheader("ğŸ¬ Live Recording with Real-time Analysis")
    
    # Recording settings
    col1, col2 = st.columns(2)
    with col1:
        duration = st.slider("Duration (seconds)", 3, 30, 10)
        show_landmarks = st.checkbox("ğŸ” Show facial landmarks", value=True)
    
    with col2:
        show_aus = st.checkbox("ğŸ“Š Show Action Units", value=True)
        confidence_threshold = st.slider("Detection confidence", 0.1, 1.0, 0.5)
    
    # Record button
    if st.button("ğŸ¥ Start Recording with Analysis", type="primary"):
        video_path, analysis_data = record_with_mediapipe(
            duration, show_landmarks, show_aus, confidence_threshold
        )
        
        if video_path and analysis_data:
            st.success(f"âœ… Video recorded and analyzed!")
            
            # Display analysis results
            display_analysis_results(analysis_data)
            
            # Offer download
            with open(video_path, 'rb') as f:
                st.download_button(
                    "ğŸ“ Download Video",
                    f.read(),
                    file_name=f"mediapipe_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.mp4",
                    mime="video/mp4"
                )

def record_with_mediapipe(duration: float, show_landmarks: bool, show_aus: bool, confidence_threshold: float) -> Tuple[Optional[Path], Optional[Dict]]:
    """Record video with real-time MediaPipe analysis"""
    
    try:
        cap = cv2.VideoCapture(0)
        if not cap.isOpened():
            st.error("âŒ Cannot access camera")
            return None, None
        
        # Set camera properties
        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
        cap.set(cv2.CAP_PROP_FPS, 30)
        
        # Output file
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_path = Path(tempfile.gettempdir()) / f"mediapipe_video_{timestamp}.mp4"
        
        # Video writer
        fps = 30
        fourcc = cv2.VideoWriter_fourcc(*'mp4v')
        out = cv2.VideoWriter(str(output_path), fourcc, fps, (640, 480))
        
        # Initialize MediaPipe
        with mp_face_mesh.FaceMesh(
            max_num_faces=1,
            refine_landmarks=True,
            min_detection_confidence=confidence_threshold,
            min_tracking_confidence=confidence_threshold
        ) as face_mesh:
            
            frames_to_record = int(fps * duration)
            analysis_data = {
                'frames': [],
                'action_units': [],
                'timestamps': [],
                'emotions': []
            }
            
            # Setup UI
            preview_placeholder = st.empty()
            progress_bar = st.progress(0)
            status_text = st.empty()
            aus_placeholder = st.empty() if show_aus else None
            
            # Countdown
            status_text.info("ğŸ”´ Get ready! Starting in 3 seconds...")
            for i in range(3, 0, -1):
                ret, frame = cap.read()
                if ret:
                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    preview_placeholder.image(frame_rgb, caption=f"Starting in {i}...", width="stretch")
                time.sleep(1)
            
            status_text.success("ğŸ¬ Recording with MediaPipe analysis!")
            
            # Record and analyze
            for frame_idx in range(frames_to_record):
                ret, frame = cap.read()
                if not ret:
                    break
                
                # Convert BGR to RGB for MediaPipe
                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                
                # Process with MediaPipe
                results = face_mesh.process(rgb_frame)
                
                # Analyze frame
                frame_analysis = analyze_frame_mediapipe(rgb_frame, results)
                
                # Store analysis data
                analysis_data['frames'].append(frame_idx)
                analysis_data['action_units'].append(frame_analysis['action_units'])
                analysis_data['timestamps'].append(frame_idx / fps)
                analysis_data['emotions'].append(frame_analysis['emotion'])
                
                # Draw landmarks if requested
                if show_landmarks and results.multi_face_landmarks:
                    for face_landmarks in results.multi_face_landmarks:
                        mp_drawing.draw_landmarks(
                            rgb_frame,
                            face_landmarks,
                            mp_face_mesh.FACEMESH_CONTOURS,
                            None,
                            mp_drawing_styles.get_default_face_mesh_contours_style()
                        )
                
                # Add recording indicator
                cv2.circle(rgb_frame, (30, 30), 10, (255, 0, 0), -1)
                
                # Write frame to video (convert back to BGR)
                bgr_frame = cv2.cvtColor(rgb_frame, cv2.COLOR_RGB2BGR)
                out.write(bgr_frame)
                
                # Update preview every 5 frames
                if frame_idx % 5 == 0:
                    current_time = frame_idx / fps
                    remaining = duration - current_time
                    
                    preview_placeholder.image(
                        rgb_frame,
                        caption=f"ğŸ”´ Recording: {current_time:.1f}s / {duration:.1f}s (Remaining: {remaining:.1f}s)",
                        width="stretch"
                    )
                
                # Update Action Units display
                if show_aus and aus_placeholder and frame_analysis['action_units']:
                    update_aus_display(aus_placeholder, frame_analysis['action_units'])
                
                progress_bar.progress((frame_idx + 1) / frames_to_record)
            
            # Cleanup
            cap.release()
            out.release()
            
            preview_placeholder.empty()
            progress_bar.empty()
            status_text.success("âœ… Recording and analysis completed!")
            
            return output_path if output_path.exists() else None, analysis_data
            
    except Exception as e:
        st.error(f"âŒ Recording failed: {str(e)}")
        return None, None

def analyze_frame_mediapipe(rgb_frame: np.ndarray, results) -> Dict:
    """Analyze a single frame using MediaPipe landmarks"""
    
    analysis = {
        'action_units': {},
        'emotion': 'neutral',
        'confidence': 0.0
    }
    
    if not results.multi_face_landmarks:
        return analysis
    
    # Get first face landmarks
    landmarks = results.multi_face_landmarks[0]
    
    # Convert landmarks to numpy array
    face_landmarks = []
    for landmark in landmarks.landmark:
        face_landmarks.append([landmark.x, landmark.y, landmark.z])
    face_landmarks = np.array(face_landmarks)
    
    # Calculate Action Units based on landmark positions
    action_units = calculate_action_units(face_landmarks)
    analysis['action_units'] = action_units
    
    # Estimate emotion based on Action Units
    emotion = estimate_emotion_from_aus(action_units)
    analysis['emotion'] = emotion
    
    # Calculate confidence based on landmark quality
    analysis['confidence'] = calculate_confidence(face_landmarks)
    
    return analysis

def calculate_action_units(landmarks: np.ndarray) -> Dict[str, float]:
    """Calculate Action Units from MediaPipe landmarks"""
    
    aus = {}
    
    try:
        # Key landmark indices (MediaPipe 468 face landmarks)
        # Based on FACS Action Unit definitions
        
        # AU1 - Inner Brow Raiser
        inner_brow_left = landmarks[70]  # Left inner eyebrow
        inner_brow_right = landmarks[55]  # Right inner eyebrow
        eye_center = (landmarks[33] + landmarks[133]) / 2  # Eye center reference
        au1_intensity = max(0, (inner_brow_left[1] + inner_brow_right[1]) / 2 - eye_center[1]) * 10
        aus['AU1_Inner_Brow_Raiser'] = min(1.0, au1_intensity)
        
        # AU2 - Outer Brow Raiser
        outer_brow_left = landmarks[70]
        outer_brow_right = landmarks[55]
        temple_ref = (landmarks[21] + landmarks[251]) / 2
        au2_intensity = max(0, (outer_brow_left[1] + outer_brow_right[1]) / 2 - temple_ref[1]) * 8
        aus['AU2_Outer_Brow_Raiser'] = min(1.0, au2_intensity)
        
        # AU4 - Brow Lowerer
        brow_center = (landmarks[9] + landmarks[10]) / 2
        forehead_ref = landmarks[10]
        au4_intensity = max(0, forehead_ref[1] - brow_center[1]) * 12
        aus['AU4_Brow_Lowerer'] = min(1.0, au4_intensity)
        
        # AU5 - Upper Lid Raiser
        upper_lid_left = landmarks[159]
        upper_lid_right = landmarks[386]
        eye_opening = abs(upper_lid_left[1] - landmarks[145][1]) + abs(upper_lid_right[1] - landmarks[374][1])
        au5_intensity = max(0, eye_opening - 0.02) * 15
        aus['AU5_Upper_Lid_Raiser'] = min(1.0, au5_intensity)
        
        # AU6 - Cheek Raiser
        cheek_left = landmarks[116]
        cheek_right = landmarks[345]
        cheek_base_left = landmarks[135]
        cheek_base_right = landmarks[364]
        cheek_raise = (abs(cheek_left[1] - cheek_base_left[1]) + abs(cheek_right[1] - cheek_base_right[1])) / 2
        au6_intensity = max(0, cheek_raise - 0.01) * 20
        aus['AU6_Cheek_Raiser'] = min(1.0, au6_intensity)
        
        # AU7 - Lid Tightener
        eye_squeeze = 1.0 - eye_opening / 0.03  # Inverse of eye opening
        aus['AU7_Lid_Tightener'] = max(0, min(1.0, eye_squeeze))
        
        # AU9 - Nose Wrinkler
        nose_tip = landmarks[19]
        nose_base = landmarks[20]
        nose_scrunch = abs(nose_tip[1] - nose_base[1])
        au9_intensity = max(0, 0.015 - nose_scrunch) * 30
        aus['AU9_Nose_Wrinkler'] = min(1.0, au9_intensity)
        
        # AU10 - Upper Lip Raiser
        upper_lip = landmarks[13]
        lip_base = landmarks[14]
        lip_raise = max(0, lip_base[1] - upper_lip[1]) * 25
        aus['AU10_Upper_Lip_Raiser'] = min(1.0, lip_raise)
        
        # AU12 - Lip Corner Puller (Smile)
        mouth_left = landmarks[61]
        mouth_right = landmarks[291]
        mouth_center = landmarks[13]
        smile_width = abs(mouth_left[0] - mouth_right[0])
        smile_intensity = max(0, smile_width - 0.06) * 8
        aus['AU12_Lip_Corner_Puller'] = min(1.0, smile_intensity)
        
        # AU15 - Lip Corner Depressor
        mouth_corners_down = max(0, mouth_center[1] - (mouth_left[1] + mouth_right[1]) / 2) * 15
        aus['AU15_Lip_Corner_Depressor'] = min(1.0, mouth_corners_down)
        
        # AU17 - Chin Raiser
        chin = landmarks[18]
        chin_base = landmarks[175]
        chin_raise = max(0, chin_base[1] - chin[1]) * 20
        aus['AU17_Chin_Raiser'] = min(1.0, chin_raise)
        
        # AU20 - Lip Stretcher
        lip_stretch = smile_width * 6
        aus['AU20_Lip_Stretcher'] = min(1.0, lip_stretch)
        
        # AU23 - Lip Tightener
        lip_area = calculate_lip_area(landmarks)
        lip_tightness = max(0, 0.001 - lip_area) * 500
        aus['AU23_Lip_Tightener'] = min(1.0, lip_tightness)
        
        # AU25 - Lips Part
        upper_lip_inner = landmarks[13]
        lower_lip_inner = landmarks[14]
        lip_separation = abs(upper_lip_inner[1] - lower_lip_inner[1])
        au25_intensity = max(0, lip_separation - 0.005) * 30
        aus['AU25_Lips_Part'] = min(1.0, au25_intensity)
        
        # AU26 - Jaw Drop
        jaw_bottom = landmarks[18]
        jaw_ref = landmarks[175]
        jaw_drop = max(0, jaw_bottom[1] - jaw_ref[1]) * 25
        aus['AU26_Jaw_Drop'] = min(1.0, jaw_drop)
        
        # AU27 - Mouth Stretch
        mouth_height = abs(landmarks[13][1] - landmarks[14][1])
        mouth_stretch = max(0, mouth_height - 0.01) * 20
        aus['AU27_Mouth_Stretch'] = min(1.0, mouth_stretch)
        
    except Exception as e:
        st.warning(f"AU calculation error: {e}")
        # Return baseline values
        au_names = [
            'AU1_Inner_Brow_Raiser', 'AU2_Outer_Brow_Raiser', 'AU4_Brow_Lowerer',
            'AU5_Upper_Lid_Raiser', 'AU6_Cheek_Raiser', 'AU7_Lid_Tightener',
            'AU9_Nose_Wrinkler', 'AU10_Upper_Lip_Raiser', 'AU12_Lip_Corner_Puller',
            'AU15_Lip_Corner_Depressor', 'AU17_Chin_Raiser', 'AU20_Lip_Stretcher',
            'AU23_Lip_Tightener', 'AU25_Lips_Part', 'AU26_Jaw_Drop', 'AU27_Mouth_Stretch'
        ]
        aus = {name: 0.0 for name in au_names}
    
    return aus

def calculate_lip_area(landmarks: np.ndarray) -> float:
    """Calculate approximate lip area from landmarks"""
    try:
        # Outer lip landmarks (approximate)
        lip_points = [landmarks[i] for i in [61, 84, 17, 314, 405, 320, 307, 375, 321, 308, 324, 318]]
        
        # Simple polygon area calculation
        x = [p[0] for p in lip_points]
        y = [p[1] for p in lip_points]
        
        area = 0.5 * abs(sum(x[i] * y[i + 1] - x[i + 1] * y[i] for i in range(-1, len(x) - 1)))
        return area
    except:
        return 0.001

def calculate_confidence(landmarks: np.ndarray) -> float:
    """Calculate detection confidence based on landmark quality"""
    try:
        # Check landmark distribution and stability
        x_coords = landmarks[:, 0]
        y_coords = landmarks[:, 1]
        
        # Higher variance usually indicates better detection
        x_var = np.var(x_coords)
        y_var = np.var(y_coords)
        
        # Normalize to 0-1 range
        confidence = min(1.0, (x_var + y_var) * 50)
        return max(0.1, confidence)
    except:
        return 0.5

def estimate_emotion_from_aus(action_units: Dict[str, float]) -> str:
    """Estimate emotion from Action Units using FACS rules"""
    
    if not action_units:
        return 'neutral'
    
    try:
        # Get AU intensities
        au12 = action_units.get('AU12_Lip_Corner_Puller', 0)
        au6 = action_units.get('AU6_Cheek_Raiser', 0)
        au4 = action_units.get('AU4_Brow_Lowerer', 0)
        au15 = action_units.get('AU15_Lip_Corner_Depressor', 0)
        au1 = action_units.get('AU1_Inner_Brow_Raiser', 0)
        au2 = action_units.get('AU2_Outer_Brow_Raiser', 0)
        au5 = action_units.get('AU5_Upper_Lid_Raiser', 0)
        au26 = action_units.get('AU26_Jaw_Drop', 0)
        au25 = action_units.get('AU25_Lips_Part', 0)
        
        # Emotion classification based on FACS combinations
        if au12 > 0.3 and au6 > 0.2:
            return 'joy'
        elif au4 > 0.3 and au15 > 0.2:
            return 'anger'
        elif au1 > 0.3 and au4 > 0.2 and au15 > 0.2:
            return 'sadness'
        elif au1 > 0.3 and au2 > 0.3 and au5 > 0.3:
            return 'surprise'
        elif au1 > 0.3 and au2 > 0.2 and au4 > 0.2:
            return 'fear'
        elif au4 > 0.2:
            return 'disgust'
        elif au26 > 0.3 or au25 > 0.3:
            return 'surprise'
        else:
            return 'neutral'
            
    except Exception:
        return 'neutral'

def update_aus_display(placeholder, action_units: Dict[str, float]):
    """Update real-time Action Units display"""
    
    if not action_units:
        return
    
    # Create a compact AU display
    au_data = []
    for au_name, intensity in action_units.items():
        if intensity > 0.1:  # Only show active AUs
            au_data.append({
                'Action Unit': au_name.replace('_', ' '),
                'Intensity': f"{intensity:.2f}",
                'Bar': 'â–ˆ' * int(intensity * 10)
            })
    
    if au_data:
        df = pd.DataFrame(au_data)
        placeholder.dataframe(df, width="stretch", hide_index=True)

def render_action_units_interface():
    """Interface for detailed Action Units analysis"""
    
    st.subheader("ğŸ“Š Action Units Analysis")
    st.markdown("Upload a video or use recorded data for detailed facial expression analysis")
    
    uploaded_file = st.file_uploader("Choose a video file", type=['mp4', 'avi', 'mov'])
    
    if uploaded_file is not None:
        # Save uploaded file
        temp_path = Path(tempfile.gettempdir()) / f"uploaded_{uploaded_file.name}"
        with open(temp_path, 'wb') as f:
            f.write(uploaded_file.read())
        
        if st.button("ğŸ”¬ Analyze Video"):
            analyze_uploaded_video(temp_path)
    
    # Show example AU information
    with st.expander("ğŸ“š Action Units Reference Guide"):
        show_au_reference()

def analyze_uploaded_video(video_path: Path):
    """Analyze uploaded video with MediaPipe"""
    
    try:
        cap = cv2.VideoCapture(str(video_path))
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        fps = cap.get(cv2.CAP_PROP_FPS)
        
        progress_bar = st.progress(0)
        status_text = st.text("Starting analysis...")
        
        analysis_results = []
        
        with mp_face_mesh.FaceMesh(
            max_num_faces=1,
            refine_landmarks=True,
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5
        ) as face_mesh:
            
            frame_count = 0
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                # Process frame
                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                results = face_mesh.process(rgb_frame)
                
                # Analyze frame
                frame_analysis = analyze_frame_mediapipe(rgb_frame, results)
                frame_analysis['frame'] = frame_count
                frame_analysis['timestamp'] = frame_count / fps
                analysis_results.append(frame_analysis)
                
                frame_count += 1
                progress_bar.progress(frame_count / total_frames)
                
                if frame_count % 30 == 0:
                    status_text.text(f"Analyzing frame {frame_count}/{total_frames}")
        
        cap.release()
        progress_bar.empty()
        status_text.empty()
        
        # Display results
        if analysis_results:
            st.success(f"âœ… Analysis complete! Processed {len(analysis_results)} frames")
            display_video_analysis_results(analysis_results)
        else:
            st.warning("No faces detected in the video")
            
    except Exception as e:
        st.error(f"Analysis failed: {str(e)}")

def display_analysis_results(analysis_data: Dict):
    """Display analysis results from recorded video"""
    
    if not analysis_data or not analysis_data['action_units']:
        st.warning("No analysis data available")
        return
    
    st.subheader("ğŸ“Š Analysis Results")
    
    # Create DataFrame
    frames_data = []
    for i, aus in enumerate(analysis_data['action_units']):
        frame_data = {
            'Frame': analysis_data['frames'][i],
            'Timestamp': analysis_data['timestamps'][i],
            'Emotion': analysis_data['emotions'][i]
        }
        frame_data.update(aus)
        frames_data.append(frame_data)
    
    df = pd.DataFrame(frames_data)
    
    # Show summary statistics
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Total Frames", len(df))
    with col2:
        dominant_emotion = df['Emotion'].mode()[0] if not df['Emotion'].empty else 'neutral'
        st.metric("Dominant Emotion", dominant_emotion)
    with col3:
        avg_au12 = df['AU12_Lip_Corner_Puller'].mean() if 'AU12_Lip_Corner_Puller' in df else 0
        st.metric("Avg Smile Intensity", f"{avg_au12:.3f}")
    
    # Show AU trends
    display_au_trends(df)

def display_video_analysis_results(analysis_results: List[Dict]):
    """Display results from video analysis"""
    
    # Convert to DataFrame
    data = []
    for result in analysis_results:
        row = {
            'Frame': result['frame'],
            'Timestamp': result['timestamp'],
            'Emotion': result['emotion'],
            'Confidence': result['confidence']
        }
        row.update(result['action_units'])
        data.append(row)
    
    df = pd.DataFrame(data)
    
    # Summary statistics
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("Total Frames", len(df))
    with col2:
        avg_confidence = df['Confidence'].mean()
        st.metric("Avg Confidence", f"{avg_confidence:.2f}")
    with col3:
        dominant_emotion = df['Emotion'].mode()[0] if not df['Emotion'].empty else 'neutral'
        st.metric("Dominant Emotion", dominant_emotion)
    with col4:
        smile_frames = len(df[df['Emotion'] == 'joy'])
        st.metric("Smile Detection", f"{smile_frames} frames")
    
    # Show detailed analysis
    display_au_trends(df)
    
    # Download results
    csv_data = df.to_csv(index=False)
    st.download_button(
        "ğŸ“„ Download Analysis CSV",
        csv_data,
        f"mediapipe_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
        "text/csv"
    )

def display_au_trends(df: pd.DataFrame):
    """Display Action Units trends over time"""
    
    st.subheader("ğŸ“ˆ Action Units Over Time")
    
    # Select which AUs to plot
    au_columns = [col for col in df.columns if col.startswith('AU')]
    
    if not au_columns:
        st.warning("No Action Units data available")
        return
    
    selected_aus = st.multiselect(
        "Select Action Units to display:",
        au_columns,
        default=au_columns[:5] if len(au_columns) >= 5 else au_columns
    )
    
    if selected_aus:
        # Create line chart
        chart_data = df[['Timestamp'] + selected_aus].set_index('Timestamp')
        st.line_chart(chart_data)
        
        # Show AU statistics
        st.subheader("ğŸ“Š Action Units Statistics")
        au_stats = df[selected_aus].describe()
        st.dataframe(au_stats, width="stretch")
        
        # Show emotion timeline
        st.subheader("ğŸ­ Emotion Timeline")
        emotion_chart = df[['Timestamp', 'Emotion']].copy()
        emotion_counts = emotion_chart['Emotion'].value_counts()
        
        col1, col2 = st.columns(2)
        with col1:
            st.bar_chart(emotion_counts)
        with col2:
            st.write("**Emotion Distribution:**")
            for emotion, count in emotion_counts.items():
                percentage = (count / len(df)) * 100
                st.write(f"- **{emotion}**: {count} frames ({percentage:.1f}%)")

def render_analytics_dashboard():
    """Analytics dashboard for processed data"""
    
    st.subheader("ğŸ“ˆ Analytics Dashboard")
    st.markdown("Historical analysis and comparison tools")
    
    # Look for processed files
    temp_dir = Path(tempfile.gettempdir())
    video_files = list(temp_dir.glob("mediapipe_video_*.mp4"))
    
    if video_files:
        st.info(f"Found {len(video_files)} recorded videos available for analysis")
        
        selected_file = st.selectbox("Select video for analysis:", video_files, format_func=lambda x: x.name)
        
        if st.button("ğŸ“Š Analyze Selected Video"):
            analyze_uploaded_video(selected_file)
    else:
        st.info("No recorded videos found. Record some videos first to see analytics!")
    
    # Show analytics features
    with st.expander("ğŸ”¬ Advanced Analytics Features"):
        st.markdown("""
        **Available Analytics:**
        - Action Units intensity tracking over time
        - Emotion classification with confidence scores
        - Facial expression pattern analysis
        - Statistical summaries and comparisons
        - Exportable CSV data for further analysis
        
        **MediaPipe Capabilities:**
        - 468 facial landmarks detection
        - Real-time processing
        - High accuracy emotion recognition
        - Professional-grade Action Units mapping
        """)

def show_au_reference():
    """Show Action Units reference guide"""
    
    st.markdown("""
    **Facial Action Coding System (FACS) - Action Units Reference:**
    
    | AU | Name | Description | Associated Emotion |
    |---|---|---|---|
    | AU1 | Inner Brow Raiser | Raises inner portion of eyebrows | Surprise, Sadness |
    | AU2 | Outer Brow Raiser | Raises outer portion of eyebrows | Surprise, Fear |
    | AU4 | Brow Lowerer | Lowers and draws eyebrows together | Anger, Concentration |
    | AU5 | Upper Lid Raiser | Raises upper eyelids | Surprise, Fear |
    | AU6 | Cheek Raiser | Raises cheeks, causes crow's feet | Joy, Genuine Smile |
    | AU7 | Lid Tightener | Tightens eyelids | Anger, Disgust |
    | AU9 | Nose Wrinkler | Wrinkles nose | Disgust |
    | AU10 | Upper Lip Raiser | Raises upper lip | Disgust, Contempt |
    | AU12 | Lip Corner Puller | Pulls lip corners up (smile) | Joy, Happiness |
    | AU15 | Lip Corner Depressor | Pulls lip corners down | Sadness |
    | AU17 | Chin Raiser | Raises and pushes up chin | Doubt, Sadness |
    | AU20 | Lip Stretcher | Stretches lips horizontally | Fear |
    | AU23 | Lip Tightener | Tightens lips | Anger |
    | AU25 | Lips Part | Separates lips | Surprise, Concentration |
    | AU26 | Jaw Drop | Lowers jaw, opens mouth | Surprise, Shock |
    | AU27 | Mouth Stretch | Stretches mouth open | Fear, Surprise |
    
    **Emotion Combinations:**
    - **Joy**: AU12 + AU6 (Smile + Cheek Raiser)
    - **Anger**: AU4 + AU7 + AU23 (Brow Lower + Lid Tighten + Lip Tighten)
    - **Surprise**: AU1 + AU2 + AU5 + AU26 (Brow Raise + Lid Raise + Jaw Drop)
    - **Fear**: AU1 + AU2 + AU4 + AU5 + AU20 (Mixed brow + Lid Raise + Lip Stretch)
    - **Sadness**: AU1 + AU4 + AU15 (Inner Brow + Brow Lower + Lip Corner Down)
    - **Disgust**: AU9 + AU10 (Nose Wrinkle + Upper Lip Raise)
    """)